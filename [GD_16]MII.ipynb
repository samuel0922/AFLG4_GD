{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[GD_16]MII.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# [GD_16]MII\n",
        "## 개요\n",
        "\n",
        "\n",
        "## 목차\n",
        "\n",
        "* STEP 0. 환경설정\n",
        "* STEP 1. 데이터준비\n",
        "* STEP 2\n",
        "* STEP 3\n",
        "* STEP 4\n",
        "* STEP 5\n",
        "\n",
        "* 루브릭\n",
        "* 회고"
      ],
      "metadata": {
        "id": "pddXSsKpdIWG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 0. 환경설정"
      ],
      "metadata": {
        "id": "6u-pTe5DdP2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Cw3__tksdP5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 라이브러리 임포트 및 환경변수 설정\n",
        "\n",
        "# 주의! ray를 tensorflow보다 먼저 import하면 오류가 발생할 수 있습니다\n",
        "import io, json, os, math\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Add, Concatenate, Lambda\n",
        "from tensorflow.keras.layers import Input, Conv2D, ReLU, MaxPool2D\n",
        "from tensorflow.keras.layers import UpSampling2D, ZeroPadding2D\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "import ray\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "PROJECT_PATH = os.getenv('HOME') + '/aiffel/mpii'\n",
        "IMAGE_PATH = os.path.join(PROJECT_PATH, 'images')\n",
        "MODEL_PATH = os.path.join(PROJECT_PATH, 'models')\n",
        "TFRECORD_PATH = os.path.join(PROJECT_PATH, 'tfrecords_mpii')\n",
        "TRAIN_JSON = os.path.join(PROJECT_PATH, 'mpii_human_pose_v1_u12_2', 'train.json')\n",
        "VALID_JSON = os.path.join(PROJECT_PATH, 'mpii_human_pose_v1_u12_2', 'validation.json')\n"
      ],
      "metadata": {
        "id": "XUh6ntfWdP8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 불러오기\n",
        "\n",
        "with open(TRAIN_JSON) as train_json:\n",
        "    train_annos = json.load(train_json)\n",
        "    json_formatted_str = json.dumps(train_annos[0], indent=2)\n",
        "    print(json_formatted_str)\n"
      ],
      "metadata": {
        "id": "unSTLmFqePa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# json annotation을 파싱하는 함수\n",
        "\n",
        "def parse_one_annotation(anno, image_dir):\n",
        "    filename = anno['image']\n",
        "    joints = anno['joints']\n",
        "    joints_visibility = anno['joints_vis']\n",
        "    annotation = {\n",
        "        'filename': filename,\n",
        "        'filepath': os.path.join(image_dir, filename),\n",
        "        'joints_visibility': joints_visibility,\n",
        "        'joints': joints,\n",
        "        'center': anno['center'],\n",
        "        'scale' : anno['scale']\n",
        "    }\n",
        "    return annotation"
      ],
      "metadata": {
        "id": "wheRapKleO7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 1. 데이터 준비"
      ],
      "metadata": {
        "id": "51MYdv3ndP_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "\n",
        "with open(TRAIN_JSON) as train_json:\n",
        "    train_annos = json.load(train_json)\n",
        "    test = parse_one_annotation(train_annos[0], IMAGE_PATH)\n",
        "    print(test)"
      ],
      "metadata": {
        "id": "lF2094VQdQCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tf.example 만드는 함수\n",
        "\n",
        "def generate_tfexample(anno):\n",
        "\n",
        "    # byte 인코딩을 위한 함수\n",
        "    def _bytes_feature(value):\n",
        "        if isinstance(value, type(tf.constant(0))):\n",
        "            value = value.numpy()\n",
        "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
        "\n",
        "    filename = anno['filename']\n",
        "    filepath = anno['filepath']\n",
        "    with open(filepath, 'rb') as image_file:\n",
        "        content = image_file.read()\n",
        "\n",
        "    image = Image.open(filepath)\n",
        "    if image.format != 'JPEG' or image.mode != 'RGB':\n",
        "        image_rgb = image.convert('RGB')\n",
        "        with io.BytesIO() as output:\n",
        "            image_rgb.save(output, format=\"JPEG\", quality=95)\n",
        "            content = output.getvalue()\n",
        "\n",
        "    width, height = image.size\n",
        "    depth = 3\n",
        "\n",
        "    c_x = int(anno['center'][0])\n",
        "    c_y = int(anno['center'][1])\n",
        "    scale = anno['scale']\n",
        "\n",
        "    x = [\n",
        "        int(joint[0]) if joint[0] >= 0 else int(joint[0]) \n",
        "        for joint in anno['joints']\n",
        "    ]\n",
        "    y = [\n",
        "        int(joint[1]) if joint[1] >= 0 else int(joint[0]) \n",
        "        for joint in anno['joints']\n",
        "    ]\n",
        "\n",
        "    v = [0 if joint_v == 0 else 2 for joint_v in anno['joints_visibility']]\n",
        "\n",
        "    feature = {\n",
        "        'image/height':\n",
        "        tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n",
        "        'image/width':\n",
        "        tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),\n",
        "        'image/depth':\n",
        "        tf.train.Feature(int64_list=tf.train.Int64List(value=[depth])),\n",
        "        'image/object/parts/x':\n",
        "        tf.train.Feature(int64_list=tf.train.Int64List(value=x)),\n",
        "        'image/object/parts/y':\n",
        "        tf.train.Feature(int64_list=tf.train.Int64List(value=y)),\n",
        "        'image/object/center/x': \n",
        "        tf.train.Feature(int64_list=tf.train.Int64List(value=[c_x])),\n",
        "        'image/object/center/y': \n",
        "        tf.train.Feature(int64_list=tf.train.Int64List(value=[c_y])),\n",
        "        'image/object/scale':\n",
        "        tf.train.Feature(float_list=tf.train.FloatList(value=[scale])),\n",
        "        'image/object/parts/v':\n",
        "        tf.train.Feature(int64_list=tf.train.Int64List(value=v)),\n",
        "        'image/encoded':\n",
        "        _bytes_feature(content),\n",
        "        'image/filename':\n",
        "        _bytes_feature(filename.encode())\n",
        "    }\n",
        "\n",
        "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "\n"
      ],
      "metadata": {
        "id": "SBiddkyhdQMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전체 데이터 l을 n개의 그룹으로 나눌지 결정\n",
        "\n",
        "def chunkify(l, n):\n",
        "    size = len(l) // n\n",
        "    start = 0\n",
        "    results = []\n",
        "    for i in range(n):\n",
        "        results.append(l[start:start + size])\n",
        "        start += size\n",
        "    return results"
      ],
      "metadata": {
        "id": "Cfd8KLJKdQPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tfrecord 생성\n",
        "\n",
        "test_chunks = chunkify([0] * 1000, 64)\n",
        "print(test_chunks)\n",
        "print(len(test_chunks))\n",
        "print(len(test_chunks[0]))"
      ],
      "metadata": {
        "id": "hz2DHt0oQqb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전체 데이터를 적당한 수의 TFRecord 파일로 만들어 주는 함수\n",
        "# ray 패키지로 병렬 처리\n",
        "\n",
        "@ray.remote\n",
        "def build_single_tfrecord(chunk, path):\n",
        "    print('start to build tf records for ' + path)\n",
        "\n",
        "    with tf.io.TFRecordWriter(path) as writer:\n",
        "        for anno in chunk:\n",
        "            tf_example = generate_tfexample(anno)\n",
        "            writer.write(tf_example.SerializeToString())\n",
        "\n",
        "    print('finished building tf records for ' + path)"
      ],
      "metadata": {
        "id": "4UOP-T-oQqy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TFRecord 생성\n",
        "\n",
        "def build_tf_records(annotations, total_shards, split):\n",
        "    chunks = chunkify(annotations, total_shards)\n",
        "    futures = [\n",
        "        build_single_tfrecord.remote(\n",
        "            chunk, '{}/{}_{}_of_{}.tfrecords'.format(\n",
        "                TFRECORD_PATH,\n",
        "                split,\n",
        "                str(i + 1).zfill(4),\n",
        "                str(total_shards).zfill(4),\n",
        "            )) for i, chunk in enumerate(chunks)\n",
        "    ]\n",
        "    ray.get(futures)\n"
      ],
      "metadata": {
        "id": "h6IJQfgkQq4r"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "\n",
        "num_train_shards = 64\n",
        "num_val_shards = 8\n",
        "\n",
        "ray.init()\n",
        "\n",
        "print('Start to parse annotations.')\n",
        "if not os.path.exists(TFRECORD_PATH):\n",
        "    os.makedirs(TFRECORD_PATH)\n",
        "\n",
        "with open(TRAIN_JSON) as train_json:\n",
        "    train_annos = json.load(train_json)\n",
        "    train_annotations = [\n",
        "        parse_one_annotation(anno, IMAGE_PATH)\n",
        "        for anno in train_annos\n",
        "    ]\n",
        "    print('First train annotation: ', train_annotations[0])\n",
        "\n",
        "with open(VALID_JSON) as val_json:\n",
        "    val_annos = json.load(val_json)\n",
        "    val_annotations = [\n",
        "        parse_one_annotation(anno, IMAGE_PATH) \n",
        "        for anno in val_annos\n",
        "    ]\n",
        "    print('First val annotation: ', val_annotations[0])\n",
        "    \n",
        "print('Start to build TF Records.')\n",
        "build_tf_records(train_annotations, num_train_shards, 'train')\n",
        "build_tf_records(val_annotations, num_val_shards, 'val')\n",
        "\n",
        "print('Successfully wrote {} annotations to TF Records.'.format(\n",
        "    len(train_annotations) + len(val_annotations)))"
      ],
      "metadata": {
        "id": "DmlZ_jVQQq8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mFqY6ChfQrAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 2. "
      ],
      "metadata": {
        "id": "_iHDFdtedQSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TFRecord로 저장된 데이터를 모델 학습에 필요한 데이터로 바꿔주는 함수\n",
        "\n",
        "def parse_tfexample(example):\n",
        "    image_feature_description = {\n",
        "        'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'image/depth': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'image/object/parts/x': tf.io.VarLenFeature(tf.int64),\n",
        "        'image/object/parts/y': tf.io.VarLenFeature(tf.int64),\n",
        "        'image/object/parts/v': tf.io.VarLenFeature(tf.int64),\n",
        "        'image/object/center/x': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'image/object/center/y': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'image/object/scale': tf.io.FixedLenFeature([], tf.float32),\n",
        "        'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
        "        'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
        "    }\n",
        "    return tf.io.parse_single_example(example, image_feature_description)\n",
        "\n"
      ],
      "metadata": {
        "id": "SL91Jcg7dQVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이미지 크롭\n",
        "\n",
        "def crop_roi(image, features, margin=0.2):\n",
        "    img_shape = tf.shape(image)\n",
        "    img_height = img_shape[0]\n",
        "    img_width = img_shape[1]\n",
        "    img_depth = img_shape[2]\n",
        "\n",
        "    keypoint_x = tf.cast(tf.sparse.to_dense(features['image/object/parts/x']), dtype=tf.int32)\n",
        "    keypoint_y = tf.cast(tf.sparse.to_dense(features['image/object/parts/y']), dtype=tf.int32)\n",
        "    center_x = features['image/object/center/x']\n",
        "    center_y = features['image/object/center/y']\n",
        "    body_height = features['image/object/scale'] * 200.0\n",
        "\n",
        "    # keypoint 중 유효한값(visible = 1) 만 사용합니다.\n",
        "    masked_keypoint_x = tf.boolean_mask(keypoint_x, keypoint_x > 0)\n",
        "    masked_keypoint_y = tf.boolean_mask(keypoint_y, keypoint_y > 0)\n",
        "\n",
        "    # min, max 값을 찾습니다.\n",
        "    keypoint_xmin = tf.reduce_min(masked_keypoint_x)\n",
        "    keypoint_xmax = tf.reduce_max(masked_keypoint_x)\n",
        "    keypoint_ymin = tf.reduce_min(masked_keypoint_y)\n",
        "    keypoint_ymax = tf.reduce_max(masked_keypoint_y)\n",
        "\n",
        "    # 높이 값을 이용해서 x, y 위치를 재조정 합니다. 박스를 정사각형으로 사용하기 위해 아래와 같이 사용합니다.\n",
        "    xmin = keypoint_xmin - tf.cast(body_height * margin, dtype=tf.int32)\n",
        "    xmax = keypoint_xmax + tf.cast(body_height * margin, dtype=tf.int32)\n",
        "    ymin = keypoint_ymin - tf.cast(body_height * margin, dtype=tf.int32)\n",
        "    ymax = keypoint_ymax + tf.cast(body_height * margin, dtype=tf.int32)\n",
        "\n",
        "    # 이미지 크기를 벗어나는 점을 재조정 해줍니다.\n",
        "    effective_xmin = xmin if xmin > 0 else 0\n",
        "    effective_ymin = ymin if ymin > 0 else 0\n",
        "    effective_xmax = xmax if xmax < img_width else img_width\n",
        "    effective_ymax = ymax if ymax < img_height else img_height\n",
        "    effective_height = effective_ymax - effective_ymin\n",
        "    effective_width = effective_xmax - effective_xmin\n",
        "\n",
        "    image = image[effective_ymin:effective_ymax, effective_xmin:effective_xmax, :]\n",
        "    new_shape = tf.shape(image)\n",
        "    new_height = new_shape[0]\n",
        "    new_width = new_shape[1]\n",
        "\n",
        "    effective_keypoint_x = (keypoint_x - effective_xmin) / new_width\n",
        "    effective_keypoint_y = (keypoint_y - effective_ymin) / new_height\n",
        "\n",
        "    return image, effective_keypoint_x, effective_keypoint_y"
      ],
      "metadata": {
        "id": "5A-0W9M3dQZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (x, y)좌표로 되어 있는 keypoint를 heatmap으로 변경하는 함수\n",
        "\n",
        "def generate_2d_guassian(height, width, y0, x0, visibility=2, sigma=1, scale=12):\n",
        "    heatmap = tf.zeros((height, width))\n",
        "\n",
        "    xmin = x0 - 3 * sigma\n",
        "    ymin = y0 - 3 * sigma\n",
        "    xmax = x0 + 3 * sigma\n",
        "    ymax = y0 + 3 * sigma\n",
        "    \n",
        "    if xmin >= width or ymin >= height or xmax < 0 or ymax < 0 or visibility == 0:\n",
        "        return heatmap\n",
        "\n",
        "    size = 6 * sigma + 1\n",
        "    x, y = tf.meshgrid(tf.range(0, 6 * sigma + 1, 1), tf.range(0, 6 * sigma + 1, 1), indexing='xy')\n",
        "\n",
        "    center_x = size // 2\n",
        "    center_y = size // 2\n",
        "\n",
        "    gaussian_patch = tf.cast(tf.math.exp(\n",
        "        -(tf.math.square(x - center_x) + tf.math.square(y - center_y)) / (tf.math.square(sigma) * 2)) * scale,\n",
        "                             dtype=tf.float32)\n",
        "\n",
        "    patch_xmin = tf.math.maximum(0, -xmin)\n",
        "    patch_ymin = tf.math.maximum(0, -ymin)\n",
        "    patch_xmax = tf.math.minimum(xmax, width) - xmin\n",
        "    patch_ymax = tf.math.minimum(ymax, height) - ymin\n",
        "\n",
        "    heatmap_xmin = tf.math.maximum(0, xmin)\n",
        "    heatmap_ymin = tf.math.maximum(0, ymin)\n",
        "    heatmap_xmax = tf.math.minimum(xmax, width)\n",
        "    heatmap_ymax = tf.math.minimum(ymax, height)\n",
        "\n",
        "    indices = tf.TensorArray(tf.int32, 1, dynamic_size=True)\n",
        "    updates = tf.TensorArray(tf.float32, 1, dynamic_size=True)\n",
        "\n",
        "    count = 0\n",
        "\n",
        "    for j in tf.range(patch_ymin, patch_ymax):\n",
        "        for i in tf.range(patch_xmin, patch_xmax):\n",
        "            indices = indices.write(count, [heatmap_ymin + j, heatmap_xmin + i])\n",
        "            updates = updates.write(count, gaussian_patch[j][i])\n",
        "            count += 1\n",
        "\n",
        "    heatmap = tf.tensor_scatter_nd_update(heatmap, indices.stack(), updates.stack())\n",
        "\n",
        "    return heatmap\n",
        "\n",
        "def make_heatmaps(features, keypoint_x, keypoint_y, heatmap_shape):\n",
        "    v = tf.cast(tf.sparse.to_dense(features['image/object/parts/v']), dtype=tf.float32)\n",
        "    x = tf.cast(tf.math.round(keypoint_x * heatmap_shape[0]), dtype=tf.int32)\n",
        "    y = tf.cast(tf.math.round(keypoint_y * heatmap_shape[1]), dtype=tf.int32)\n",
        "\n",
        "    num_heatmap = heatmap_shape[2]\n",
        "    heatmap_array = tf.TensorArray(tf.float32, 16)\n",
        "\n",
        "    for i in range(num_heatmap):\n",
        "        gaussian = self.generate_2d_guassian(heatmap_shape[1], heatmap_shape[0], y[i], x[i], v[i])\n",
        "        heatmap_array = heatmap_array.write(i, gaussian)\n",
        "\n",
        "    heatmaps = heatmap_array.stack()\n",
        "    heatmaps = tf.transpose(heatmaps, perm=[1, 2, 0])  # change to (64, 64, 16)\n",
        "\n",
        "    return heatmaps"
      ],
      "metadata": {
        "id": "-iA24cGMdQcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 전처리 클래스\n",
        "\n",
        "class Preprocessor(object):\n",
        "    def __init__(self,\n",
        "                 image_shape=(256, 256, 3),\n",
        "                 heatmap_shape=(64, 64, 16),\n",
        "                 is_train=False):\n",
        "        self.is_train = is_train\n",
        "        self.image_shape = image_shape\n",
        "        self.heatmap_shape = heatmap_shape\n",
        "\n",
        "    def __call__(self, example):\n",
        "        features = self.parse_tfexample(example)\n",
        "        image = tf.io.decode_jpeg(features['image/encoded'])\n",
        "\n",
        "        if self.is_train:\n",
        "            random_margin = tf.random.uniform([1], 0.1, 0.3)[0]\n",
        "            image, keypoint_x, keypoint_y = self.crop_roi(image, features, margin=random_margin)\n",
        "            image = tf.image.resize(image, self.image_shape[0:2])\n",
        "        else:\n",
        "            image, keypoint_x, keypoint_y = self.crop_roi(image, features)\n",
        "            image = tf.image.resize(image, self.image_shape[0:2])\n",
        "\n",
        "        image = tf.cast(image, tf.float32) / 127.5 - 1\n",
        "        heatmaps = self.make_heatmaps(features, keypoint_x, keypoint_y, self.heatmap_shape)\n",
        "\n",
        "        return image, heatmaps\n",
        "\n",
        "        \n",
        "    def crop_roi(self, image, features, margin=0.2):\n",
        "        img_shape = tf.shape(image)\n",
        "        img_height = img_shape[0]\n",
        "        img_width = img_shape[1]\n",
        "        img_depth = img_shape[2]\n",
        "\n",
        "        keypoint_x = tf.cast(tf.sparse.to_dense(features['image/object/parts/x']), dtype=tf.int32)\n",
        "        keypoint_y = tf.cast(tf.sparse.to_dense(features['image/object/parts/y']), dtype=tf.int32)\n",
        "        center_x = features['image/object/center/x']\n",
        "        center_y = features['image/object/center/y']\n",
        "        body_height = features['image/object/scale'] * 200.0\n",
        "        \n",
        "        masked_keypoint_x = tf.boolean_mask(keypoint_x, keypoint_x > 0)\n",
        "        masked_keypoint_y = tf.boolean_mask(keypoint_y, keypoint_y > 0)\n",
        "        \n",
        "        keypoint_xmin = tf.reduce_min(masked_keypoint_x)\n",
        "        keypoint_xmax = tf.reduce_max(masked_keypoint_x)\n",
        "        keypoint_ymin = tf.reduce_min(masked_keypoint_y)\n",
        "        keypoint_ymax = tf.reduce_max(masked_keypoint_y)\n",
        "        \n",
        "        xmin = keypoint_xmin - tf.cast(body_height * margin, dtype=tf.int32)\n",
        "        xmax = keypoint_xmax + tf.cast(body_height * margin, dtype=tf.int32)\n",
        "        ymin = keypoint_ymin - tf.cast(body_height * margin, dtype=tf.int32)\n",
        "        ymax = keypoint_ymax + tf.cast(body_height * margin, dtype=tf.int32)\n",
        "        \n",
        "        effective_xmin = xmin if xmin > 0 else 0\n",
        "        effective_ymin = ymin if ymin > 0 else 0\n",
        "        effective_xmax = xmax if xmax < img_width else img_width\n",
        "        effective_ymax = ymax if ymax < img_height else img_height\n",
        "        effective_height = effective_ymax - effective_ymin\n",
        "        effective_width = effective_xmax - effective_xmin\n",
        "\n",
        "        image = image[effective_ymin:effective_ymax, effective_xmin:effective_xmax, :]\n",
        "        new_shape = tf.shape(image)\n",
        "        new_height = new_shape[0]\n",
        "        new_width = new_shape[1]\n",
        "        \n",
        "        effective_keypoint_x = (keypoint_x - effective_xmin) / new_width\n",
        "        effective_keypoint_y = (keypoint_y - effective_ymin) / new_height\n",
        "        \n",
        "        return image, effective_keypoint_x, effective_keypoint_y\n",
        "        \n",
        "    \n",
        "    def generate_2d_guassian(self, height, width, y0, x0, visibility=2, sigma=1, scale=12):\n",
        "        \n",
        "        heatmap = tf.zeros((height, width))\n",
        "\n",
        "        xmin = x0 - 3 * sigma\n",
        "        ymin = y0 - 3 * sigma\n",
        "        xmax = x0 + 3 * sigma\n",
        "        ymax = y0 + 3 * sigma\n",
        "\n",
        "        if xmin >= width or ymin >= height or xmax < 0 or ymax <0 or visibility == 0:\n",
        "            return heatmap\n",
        "\n",
        "        size = 6 * sigma + 1\n",
        "        x, y = tf.meshgrid(tf.range(0, 6*sigma+1, 1), tf.range(0, 6*sigma+1, 1), indexing='xy')\n",
        "\n",
        "        center_x = size // 2\n",
        "        center_y = size // 2\n",
        "\n",
        "        gaussian_patch = tf.cast(tf.math.exp(-(tf.square(x - center_x) + tf.math.square(y - center_y)) / (tf.math.square(sigma) * 2)) * scale, dtype=tf.float32)\n",
        "\n",
        "        patch_xmin = tf.math.maximum(0, -xmin)\n",
        "        patch_ymin = tf.math.maximum(0, -ymin)\n",
        "        patch_xmax = tf.math.minimum(xmax, width) - xmin\n",
        "        patch_ymax = tf.math.minimum(ymax, height) - ymin\n",
        "\n",
        "        heatmap_xmin = tf.math.maximum(0, xmin)\n",
        "        heatmap_ymin = tf.math.maximum(0, ymin)\n",
        "        heatmap_xmax = tf.math.minimum(xmax, width)\n",
        "        heatmap_ymax = tf.math.minimum(ymax, height)\n",
        "\n",
        "        indices = tf.TensorArray(tf.int32, 1, dynamic_size=True)\n",
        "        updates = tf.TensorArray(tf.float32, 1, dynamic_size=True)\n",
        "\n",
        "        count = 0\n",
        "\n",
        "        for j in tf.range(patch_ymin, patch_ymax):\n",
        "            for i in tf.range(patch_xmin, patch_xmax):\n",
        "                indices = indices.write(count, [heatmap_ymin+j, heatmap_xmin+i])\n",
        "                updates = updates.write(count, gaussian_patch[j][i])\n",
        "                count += 1\n",
        "                \n",
        "        heatmap = tf.tensor_scatter_nd_update(heatmap, indices.stack(), updates.stack())\n",
        "\n",
        "        return heatmap\n",
        "\n",
        "\n",
        "    def make_heatmaps(self, features, keypoint_x, keypoint_y, heatmap_shape):\n",
        "        v = tf.cast(tf.sparse.to_dense(features['image/object/parts/v']), dtype=tf.float32)\n",
        "        x = tf.cast(tf.math.round(keypoint_x * heatmap_shape[0]), dtype=tf.int32)\n",
        "        y = tf.cast(tf.math.round(keypoint_y * heatmap_shape[1]), dtype=tf.int32)\n",
        "        \n",
        "        num_heatmap = heatmap_shape[2]\n",
        "        heatmap_array = tf.TensorArray(tf.float32, 16)\n",
        "\n",
        "        for i in range(num_heatmap):\n",
        "            gaussian = self.generate_2d_guassian(heatmap_shape[1], heatmap_shape[0], y[i], x[i], v[i])\n",
        "            heatmap_array = heatmap_array.write(i, gaussian)\n",
        "        \n",
        "        heatmaps = heatmap_array.stack()\n",
        "        heatmaps = tf.transpose(heatmaps, perm=[1, 2, 0]) # change to (64, 64, 16)\n",
        "        \n",
        "        return heatmaps\n",
        "\n",
        "    def parse_tfexample(self, example):\n",
        "        image_feature_description = {\n",
        "            'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
        "            'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
        "            'image/depth': tf.io.FixedLenFeature([], tf.int64),\n",
        "            'image/object/parts/x': tf.io.VarLenFeature(tf.int64),\n",
        "            'image/object/parts/y': tf.io.VarLenFeature(tf.int64),\n",
        "            'image/object/parts/v': tf.io.VarLenFeature(tf.int64),\n",
        "            'image/object/center/x': tf.io.FixedLenFeature([], tf.int64),\n",
        "            'image/object/center/y': tf.io.FixedLenFeature([], tf.int64),\n",
        "            'image/object/scale': tf.io.FixedLenFeature([], tf.float32),\n",
        "            'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
        "            'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
        "        }\n",
        "        return tf.io.parse_single_example(example,\n",
        "                                          image_feature_description)\n",
        "\n"
      ],
      "metadata": {
        "id": "yg_E3UXxdQfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "SOSTTxcgdQiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 3. 모델"
      ],
      "metadata": {
        "id": "KcC1feBUdQln"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### S3-1. Hourglass 구조"
      ],
      "metadata": {
        "id": "o0nqm0LTdQow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# bottle neck block\n",
        "\n",
        "def BottleneckBlock(inputs, filters, strides=1, downsample=False, name=None):\n",
        "    identity = inputs\n",
        "    if downsample:\n",
        "        identity = Conv2D(\n",
        "            filters=filters,\n",
        "            kernel_size=1,\n",
        "            strides=strides,\n",
        "            padding='same',\n",
        "            kernel_initializer='he_normal')(inputs)\n",
        "\n",
        "    x = BatchNormalization(momentum=0.9)(inputs)\n",
        "    x = ReLU()(x)\n",
        "    x = Conv2D(\n",
        "        filters=filters // 2,\n",
        "        kernel_size=1,\n",
        "        strides=1,\n",
        "        padding='same',\n",
        "        kernel_initializer='he_normal')(x)\n",
        "\n",
        "    x = BatchNormalization(momentum=0.9)(x)\n",
        "    x = ReLU()(x)\n",
        "    x = Conv2D(\n",
        "        filters=filters // 2,\n",
        "        kernel_size=3,\n",
        "        strides=strides,\n",
        "        padding='same',\n",
        "        kernel_initializer='he_normal')(x)\n",
        "\n",
        "    x = BatchNormalization(momentum=0.9)(x)\n",
        "    x = ReLU()(x)\n",
        "    x = Conv2D(\n",
        "        filters=filters,\n",
        "        kernel_size=1,\n",
        "        strides=1,\n",
        "        padding='same',\n",
        "        kernel_initializer='he_normal')(x)\n",
        "\n",
        "    x = Add()([identity, x])\n",
        "    return x"
      ],
      "metadata": {
        "id": "ktBCSFiMdQr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HourglassModule\n",
        "\n",
        "def HourglassModule(inputs, order, filters, num_residual):\n",
        "    \n",
        "    up1 = BottleneckBlock(inputs, filters, downsample=False)\n",
        "    for i in range(num_residual):\n",
        "        up1 = BottleneckBlock(up1, filters, downsample=False)\n",
        "\n",
        "    low1 = MaxPool2D(pool_size=2, strides=2)(inputs)\n",
        "    for i in range(num_residual):\n",
        "        low1 = BottleneckBlock(low1, filters, downsample=False)\n",
        "\n",
        "    low2 = low1\n",
        "    if order > 1:\n",
        "        low2 = HourglassModule(low1, order - 1, filters, num_residual)\n",
        "    else:\n",
        "        for i in range(num_residual):\n",
        "            low2 = BottleneckBlock(low2, filters, downsample=False)\n",
        "\n",
        "    low3 = low2\n",
        "    for i in range(num_residual):\n",
        "        low3 = BottleneckBlock(low3, filters, downsample=False)\n",
        "\n",
        "    up2 = UpSampling2D(size=2)(low3)\n",
        "\n",
        "    return up2 + up1"
      ],
      "metadata": {
        "id": "ZvIvxerMdQvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# intermediate output을 위한 linear layer\n",
        "\n",
        "def LinearLayer(inputs, filters):\n",
        "    x = Conv2D(\n",
        "        filters=filters,\n",
        "        kernel_size=1,\n",
        "        strides=1,\n",
        "        padding='same',\n",
        "        kernel_initializer='he_normal')(inputs)\n",
        "    x = BatchNormalization(momentum=0.9)(x)\n",
        "    x = ReLU()(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "t2omI-cHdQyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stacked hourglass 여러 층\n",
        "\n",
        "def StackedHourglassNetwork(\n",
        "        input_shape=(256, 256, 3), \n",
        "        num_stack=4, \n",
        "        num_residual=1,\n",
        "        num_heatmap=16):\n",
        "    \n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    x = Conv2D(\n",
        "        filters=64,\n",
        "        kernel_size=7,\n",
        "        strides=2,\n",
        "        padding='same',\n",
        "        kernel_initializer='he_normal')(inputs)\n",
        "    x = BatchNormalization(momentum=0.9)(x)\n",
        "    x = ReLU()(x)\n",
        "    x = BottleneckBlock(x, 128, downsample=True)\n",
        "    x = MaxPool2D(pool_size=2, strides=2)(x)\n",
        "    x = BottleneckBlock(x, 128, downsample=False)\n",
        "    x = BottleneckBlock(x, 256, downsample=True)\n",
        "\n",
        "    ys = []\n",
        "    for i in range(num_stack):\n",
        "        x = HourglassModule(x, order=4, filters=256, num_residual=num_residual)\n",
        "        for i in range(num_residual):\n",
        "            x = BottleneckBlock(x, 256, downsample=False)\n",
        "\n",
        "        x = LinearLayer(x, 256)\n",
        "\n",
        "        y = Conv2D(\n",
        "            filters=num_heatmap,\n",
        "            kernel_size=1,\n",
        "            strides=1,\n",
        "            padding='same',\n",
        "            kernel_initializer='he_normal')(x)\n",
        "        ys.append(y)\n",
        "\n",
        "        if i < num_stack - 1:\n",
        "            y_intermediate_1 = Conv2D(filters=256, kernel_size=1, strides=1)(x)\n",
        "            y_intermediate_2 = Conv2D(filters=256, kernel_size=1, strides=1)(y)\n",
        "            x = Add()([y_intermediate_1, y_intermediate_2])\n",
        "\n",
        "    return tf.keras.Model(inputs, ys, name='stacked_hourglass')\n"
      ],
      "metadata": {
        "id": "XMc4PApDdQ1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### S3-2. Simplebaseline 구조"
      ],
      "metadata": {
        "id": "GO2it8bvTtxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder : conv layers\n",
        "# backbone : ResNet\n",
        "\n",
        "resnet = tf.keras.applications.resnet.ResNet50(include_top=False, weights='imagenet')"
      ],
      "metadata": {
        "id": "ILM4rtGJTuAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder : Deconv Module + Upsampling\n",
        "# Deconv Module : deconv-bn-relu 3\n",
        "# deconv : 256filter size, 4x4 kernel, stride 2로 2배씩 feature map이 커짐\n",
        "\n",
        "def _make_deconv_layer(num_deconv_layers):\n",
        "    seq_model = tf.keras.models.Sequential()\n",
        "    for i in range(num_deconv_layers):\n",
        "        seq_model.add(tf.keras.layers.Conv2DTranspose(256, kernel_size=(2,2), strides=2))\n",
        "        seq_model.add(tf.keras.layers.BatchNormalization(momentum=0.9))\n",
        "        seq_model.add(tf.keras.layers.ReLU())\n",
        "    return seq_model"
      ],
      "metadata": {
        "id": "V1wW4SPUTuDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "upconv = _make_deconv_layer(3)"
      ],
      "metadata": {
        "id": "Z-kkMqjHTuHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 마지막 출력 레이어 : k개 1x1 conv layer\n",
        "\n",
        "final_layer = tf.keras.layers.Conv2D(16, kernel_size=(1,1), padding='same')"
      ],
      "metadata": {
        "id": "nJC967uHTuKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# simplebaseline 모델 생성\n",
        "\n",
        "def Simplebaseline(input_shape=(256, 256, 3)):\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "    x = resnet(inputs)\n",
        "    x = upconv(x)\n",
        "    out = final_layer(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs, out, name='simple_baseline')\n",
        "    return model"
      ],
      "metadata": {
        "id": "l55w9q_-Twfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 4. 학습"
      ],
      "metadata": {
        "id": "Se_auT1KdQ4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "\n",
        "class Trainer(object):\n",
        "    def __init__(self,\n",
        "                model_file_name,\n",
        "                model,\n",
        "                epochs,\n",
        "                global_batch_size,\n",
        "                strategy,\n",
        "                initial_learning_rate):\n",
        "\n",
        "        self.model_file_name = model_file_name\n",
        "        self.model = model\n",
        "        self.epochs = epochs\n",
        "        self.strategy = strategy\n",
        "        self.global_batch_size = global_batch_size\n",
        "        self.loss_object = tf.keras.losses.MeanSquaredError(\n",
        "            reduction=tf.keras.losses.Reduction.NONE)\n",
        "        self.optimizer = tf.keras.optimizers.Adam(\n",
        "            learning_rate=initial_learning_rate)\n",
        "        self.model = model\n",
        "\n",
        "        self.current_learning_rate = initial_learning_rate\n",
        "        self.last_val_loss = math.inf\n",
        "        self.lowest_val_loss = math.inf\n",
        "        self.patience_count = 0\n",
        "        self.max_patience = 10\n",
        "        self.best_model = None\n",
        "\n",
        "    def lr_decay(self):\n",
        "        if self.patience_count >= self.max_patience:\n",
        "            self.current_learning_rate /= 10.0\n",
        "            self.patience_count = 0\n",
        "        elif self.last_val_loss == self.lowest_val_loss:\n",
        "            self.patience_count = 0\n",
        "        self.patience_count += 1\n",
        "\n",
        "        self.optimizer.learning_rate = self.current_learning_rate\n",
        "\n",
        "    def lr_decay_step(self, epoch):\n",
        "        if epoch == 25 or epoch == 50 or epoch == 75:\n",
        "            self.current_learning_rate /= 10.0\n",
        "        self.optimizer.learning_rate = self.current_learning_rate\n",
        "\n",
        "    def compute_loss(self, labels, outputs):\n",
        "        loss = 0\n",
        "        for output in outputs:\n",
        "            weights = tf.cast(labels > 0, dtype=tf.float32) * 81 + 1\n",
        "            loss += tf.math.reduce_mean(\n",
        "                tf.math.square(labels - output) * weights) * (\n",
        "                    1. / self.global_batch_size)\n",
        "        return loss\n",
        "\n",
        "    def train_step(self, inputs):\n",
        "        images, labels = inputs\n",
        "        with tf.GradientTape() as tape:\n",
        "            outputs = self.model(images, training=True)\n",
        "            loss = self.compute_loss(labels, outputs)\n",
        "\n",
        "        grads = tape.gradient(\n",
        "            target=loss, sources=self.model.trainable_variables)\n",
        "        self.optimizer.apply_gradients(\n",
        "            zip(grads, self.model.trainable_variables))\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def val_step(self, inputs):\n",
        "        images, labels = inputs\n",
        "        outputs = self.model(images, training=False)\n",
        "        loss = self.compute_loss(labels, outputs)\n",
        "        return loss\n",
        "\n",
        "    def run(self, train_dist_dataset, val_dist_dataset):\n",
        "        @tf.function\n",
        "        def distributed_train_epoch(dataset):\n",
        "            tf.print('Start distributed traininng...')\n",
        "            total_loss = 0.0\n",
        "            num_train_batches = 0.0\n",
        "            for one_batch in dataset:\n",
        "                per_replica_loss = self.strategy.run(\n",
        "                    self.train_step, args=(one_batch, ))\n",
        "                batch_loss = self.strategy.reduce(\n",
        "                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
        "                total_loss += batch_loss\n",
        "                num_train_batches += 1\n",
        "                tf.print('Trained batch', num_train_batches, 'batch loss',\n",
        "                         batch_loss, 'epoch total loss', total_loss / num_train_batches)\n",
        "            return total_loss, num_train_batches\n",
        "\n",
        "        @tf.function\n",
        "        def distributed_val_epoch(dataset):\n",
        "            total_loss = 0.0\n",
        "            num_val_batches = 0.0\n",
        "            for one_batch in dataset:\n",
        "                per_replica_loss = self.strategy.run(\n",
        "                    self.val_step, args=(one_batch, ))\n",
        "                num_val_batches += 1\n",
        "                batch_loss = self.strategy.reduce(\n",
        "                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
        "                tf.print('Validated batch', num_val_batches, 'batch loss',\n",
        "                         batch_loss)\n",
        "                if not tf.math.is_nan(batch_loss):\n",
        "                    # TODO: Find out why the last validation batch loss become NaN\n",
        "                    total_loss += batch_loss\n",
        "                else:\n",
        "                    num_val_batches -= 1\n",
        "\n",
        "            return total_loss, num_val_batches\n",
        "\n",
        "        for epoch in range(1, self.epochs + 1):\n",
        "            self.lr_decay()\n",
        "            print('Start epoch {} with learning rate {}'.format(\n",
        "                epoch, self.current_learning_rate))\n",
        "\n",
        "            train_total_loss, num_train_batches = distributed_train_epoch(\n",
        "                train_dist_dataset)\n",
        "            train_loss = train_total_loss / num_train_batches\n",
        "            print('Epoch {} train loss {}'.format(epoch, train_loss))\n",
        "\n",
        "            val_total_loss, num_val_batches = distributed_val_epoch(\n",
        "                val_dist_dataset)\n",
        "            val_loss = val_total_loss / num_val_batches\n",
        "            print('Epoch {} val loss {}'.format(epoch, val_loss))\n",
        "\n",
        "            # save model when reach a new lowest validation loss\n",
        "            if val_loss < self.lowest_val_loss:\n",
        "                self.save_model(epoch, val_loss)\n",
        "                self.lowest_val_loss = val_loss\n",
        "            self.last_val_loss = val_loss\n",
        "\n",
        "        return self.best_model\n",
        "\n",
        "    def save_model(self, epoch, loss):\n",
        "        model_name = MODEL_PATH + '/{}-epoch-{}-loss-{:.4f}.h5'.format(self.model_file_name, epoch, loss)\n",
        "        self.model.save_weights(model_name)\n",
        "        self.best_model = model_name\n",
        "        print(\"Model {} saved.\".format(model_name))"
      ],
      "metadata": {
        "id": "Z28mnaaddQ8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 만드는 함수\n",
        "\n",
        "IMAGE_SHAPE = (256, 256, 3)\n",
        "HEATMAP_SIZE = (64, 64)\n",
        "\n",
        "def create_dataset(tfrecords, batch_size, num_heatmap, is_train):\n",
        "    preprocess = Preprocessor(\n",
        "        IMAGE_SHAPE, (HEATMAP_SIZE[0], HEATMAP_SIZE[1], num_heatmap), is_train)\n",
        "\n",
        "    dataset = tf.data.Dataset.list_files(tfrecords)\n",
        "    dataset = tf.data.TFRecordDataset(dataset)\n",
        "    dataset = dataset.map(\n",
        "        preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    if is_train:\n",
        "        dataset = dataset.shuffle(batch_size)\n",
        "\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "5CvNHEPCdQ_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "\n",
        "def train(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords, model_file_name='model', is_stackedhourglassnetwork=True):\n",
        "    strategy = tf.distribute.MirroredStrategy()\n",
        "    global_batch_size = strategy.num_replicas_in_sync * batch_size\n",
        "    train_dataset = create_dataset(\n",
        "        train_tfrecords, global_batch_size, num_heatmap, is_train=True)\n",
        "    val_dataset = create_dataset(\n",
        "        val_tfrecords, global_batch_size, num_heatmap, is_train=False)\n",
        "\n",
        "    if not os.path.exists(MODEL_PATH):\n",
        "        os.makedirs(MODEL_PATH)\n",
        "\n",
        "    with strategy.scope():\n",
        "        train_dist_dataset = strategy.experimental_distribute_dataset(\n",
        "            train_dataset)\n",
        "        val_dist_dataset = strategy.experimental_distribute_dataset(\n",
        "            val_dataset)\n",
        "        \n",
        "        if is_stackedhourglassnetwork:\n",
        "            model = StackedHourglassNetwork(IMAGE_SHAPE, 4, 1, num_heatmap)\n",
        "        else:\n",
        "            model = Simplebaseline(IMAGE_SHAPE)\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model_file_name,\n",
        "            model,\n",
        "            epochs,\n",
        "            global_batch_size,\n",
        "            strategy,\n",
        "            initial_learning_rate=learning_rate)\n",
        "\n",
        "        print('Start training...')\n",
        "        return trainer.run(train_dist_dataset, val_dist_dataset)\n"
      ],
      "metadata": {
        "id": "1SUQyUx6dRDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HYPER PARAMETER 설정\n",
        "\n",
        "train_tfrecords = os.path.join(TFRECORD_PATH, 'train*')\n",
        "val_tfrecords = os.path.join(TFRECORD_PATH, 'val*')\n",
        "epochs = 5\n",
        "batch_size = 16\n",
        "num_heatmap = 16\n",
        "learning_rate = 0.0007"
      ],
      "metadata": {
        "id": "IbeqXoG5dRGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  stackedhourglassNetwork 학습\n",
        "\n",
        "# best_model_SHN_file = train(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords, 'model_SHN', is_stackedhourglassnetwork=True)"
      ],
      "metadata": {
        "id": "XeXs8wqjdRJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# simplebaseline 학습\n",
        "\n",
        "# best_model_SBL_file = train(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords, 'model_SBL', is_stackedhourglassnetwork=False)"
      ],
      "metadata": {
        "id": "FqIyZcMVVAvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 5. Inference"
      ],
      "metadata": {
        "id": "RZGijwaJdRM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HYPER PARAMETER 설정\n",
        "\n",
        "R_ANKLE = 0\n",
        "R_KNEE = 1\n",
        "R_HIP = 2\n",
        "L_HIP = 3\n",
        "L_KNEE = 4\n",
        "L_ANKLE = 5\n",
        "PELVIS = 6\n",
        "THORAX = 7\n",
        "UPPER_NECK = 8\n",
        "HEAD_TOP = 9\n",
        "R_WRIST = 10\n",
        "R_ELBOW = 11\n",
        "R_SHOULDER = 12\n",
        "L_SHOULDER = 13\n",
        "L_ELBOW = 14\n",
        "L_WRIST = 15\n",
        "\n",
        "MPII_BONES = [\n",
        "    [R_ANKLE, R_KNEE],\n",
        "    [R_KNEE, R_HIP],\n",
        "    [R_HIP, PELVIS],\n",
        "    [L_HIP, PELVIS],\n",
        "    [L_HIP, L_KNEE],\n",
        "    [L_KNEE, L_ANKLE],\n",
        "    [PELVIS, THORAX],\n",
        "    [THORAX, UPPER_NECK],\n",
        "    [UPPER_NECK, HEAD_TOP],\n",
        "    [R_WRIST, R_ELBOW],\n",
        "    [R_ELBOW, R_SHOULDER],\n",
        "    [THORAX, R_SHOULDER],\n",
        "    [THORAX, L_SHOULDER],\n",
        "    [L_SHOULDER, L_ELBOW],\n",
        "    [L_ELBOW, L_WRIST]\n",
        "]\n"
      ],
      "metadata": {
        "id": "u7YjilzRd1_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "\n",
        "def find_max_coordinates(heatmaps):\n",
        "    flatten_heatmaps = tf.reshape(heatmaps, (-1, 16))\n",
        "    indices = tf.math.argmax(flatten_heatmaps, axis=0)\n",
        "    y = tf.cast(indices / 64, dtype=tf.int64)\n",
        "    x = indices - 64 * y\n",
        "    return tf.stack([x, y], axis=1).numpy()"
      ],
      "metadata": {
        "id": "e3SWhuIPd2B4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# heatmap에서 keypoint 추출\n",
        "\n",
        "def extract_keypoints_from_heatmap(heatmaps):\n",
        "    max_keypoints = find_max_coordinates(heatmaps)\n",
        "\n",
        "    padded_heatmap = np.pad(heatmaps, [[1,1],[1,1],[0,0]], mode='constant')\n",
        "    adjusted_keypoints = []\n",
        "    for i, keypoint in enumerate(max_keypoints):\n",
        "        max_y = keypoint[1]+1\n",
        "        max_x = keypoint[0]+1\n",
        "        \n",
        "        patch = padded_heatmap[max_y-1:max_y+2, max_x-1:max_x+2, i]\n",
        "        patch[1][1] = 0\n",
        "        \n",
        "        index = np.argmax(patch)\n",
        "        \n",
        "        next_y = index // 3\n",
        "        next_x = index - next_y * 3\n",
        "        delta_y = (next_y - 1) / 4\n",
        "        delta_x = (next_x - 1) / 4\n",
        "        \n",
        "        adjusted_keypoint_x = keypoint[0] + delta_x\n",
        "        adjusted_keypoint_y = keypoint[1] + delta_y\n",
        "        adjusted_keypoints.append((adjusted_keypoint_x, adjusted_keypoint_y))\n",
        "        \n",
        "    adjusted_keypoints = np.clip(adjusted_keypoints, 0, 64)\n",
        "    normalized_keypoints = adjusted_keypoints / 64\n",
        "    return normalized_keypoints"
      ],
      "metadata": {
        "id": "4ff-PEN0d2Em"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예측\n",
        "\n",
        "def predict(model, image_path):\n",
        "    encoded = tf.io.read_file(image_path)\n",
        "    image = tf.io.decode_jpeg(encoded)\n",
        "    inputs = tf.image.resize(image, (256, 256))\n",
        "    inputs = tf.cast(inputs, tf.float32) / 127.5 - 1\n",
        "    inputs = tf.expand_dims(inputs, 0)\n",
        "    outputs = model(inputs, training=False)\n",
        "    if type(outputs) != list:\n",
        "        outputs = [outputs]\n",
        "    heatmap = tf.squeeze(outputs[-1], axis=0).numpy()\n",
        "    kp = extract_keypoints_from_heatmap(heatmap)\n",
        "    return image, kp"
      ],
      "metadata": {
        "id": "vEQRgXlUd2Ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 그리기\n",
        "\n",
        "def draw_keypoints_on_image(image, keypoints, index=None):\n",
        "    fig,ax = plt.subplots(1)\n",
        "    ax.imshow(image)\n",
        "    joints = []\n",
        "    for i, joint in enumerate(keypoints):\n",
        "        joint_x = joint[0] * image.shape[1]\n",
        "        joint_y = joint[1] * image.shape[0]\n",
        "        if index is not None and index != i:\n",
        "            continue\n",
        "        plt.scatter(joint_x, joint_y, s=10, c='red', marker='o')\n",
        "    plt.show()\n",
        "\n",
        "def draw_skeleton_on_image(image, keypoints, index=None):\n",
        "    fig,ax = plt.subplots(1)\n",
        "    ax.imshow(image)\n",
        "    joints = []\n",
        "    for i, joint in enumerate(keypoints):\n",
        "        joint_x = joint[0] * image.shape[1]\n",
        "        joint_y = joint[1] * image.shape[0]\n",
        "        joints.append((joint_x, joint_y))\n",
        "    \n",
        "    for bone in MPII_BONES:\n",
        "        joint_1 = joints[bone[0]]\n",
        "        joint_2 = joints[bone[1]]\n",
        "        plt.plot([joint_1[0], joint_2[0]], [joint_1[1], joint_2[1]], linewidth=5, alpha=0.7)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "kEZYpu8sd2LD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "AVbv0aIRViKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_image = os.path.join(PROJECT_PATH, 'test_image.jpg')"
      ],
      "metadata": {
        "id": "3aa7___oViWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ANXWbUlEVia9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WEIGHTS_PATH = os.path.join(PROJECT_PATH, 'models', 'model-v0.0.1-epoch-2-loss-1.3072.h5')"
      ],
      "metadata": {
        "id": "I4AgF-4WVifM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_SHN = StackedHourglassNetwork(IMAGE_SHAPE, 4, 1)\n",
        "model_SHN.load_weights(WEIGHTS_PATH)\n",
        "\n",
        "# 이전의 학습하는 코드 블럭을 통해 학습하고 그 모델을 사용할 경우 아래 주석 처리된 코드를 사용하면 됩니다\n",
        "# model_SHN.load_weights(best_model_SHN_file)"
      ],
      "metadata": {
        "id": "icFtHWSwVijv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_SHN, keypoints_SHN = predict(model_SHN, test_image)\n",
        "draw_keypoints_on_image(image_SHN, keypoints_SHN)\n",
        "draw_skeleton_on_image(image_SHN, keypoints_SHN)"
      ],
      "metadata": {
        "id": "nSZQ0DJ5Viob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_SBL = Simplebaseline(IMAGE_SHAPE)\n",
        "model_SBL.load_weights(WEIGHTS_PATH)\n",
        "\n",
        "# 이전의 학습하는 코드 블럭을 통해 학습하고 그 모델을 사용할 경우 아래 주석 처리된 코드를 사용하면 됩니다\n",
        "# model_SBL.load_weights(best_model_SBL_file)"
      ],
      "metadata": {
        "id": "5ckhoNmSVit1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_SBL, keypoints_SBL = predict(model_SBL, test_image)\n",
        "draw_keypoints_on_image(image_SBL, keypoints_SBL)\n",
        "draw_skeleton_on_image(image_SBL, keypoints_SBL)"
      ],
      "metadata": {
        "id": "u47WwcAwVw6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[ 루브릭 ]"
      ],
      "metadata": {
        "id": "hnjxLSscdRP_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [ 회고 ]"
      ],
      "metadata": {
        "id": "gTgZ90U8dRTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "X9TR6hJteBkR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}